{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# First look at Hyperparameter Tuning\r\n",
        "\r\n",
        "This is an introduction to the hyperparameter features of the Azure Machine Learning service.  In it, you will create, register and deploy a model, then proceed to use the hyperparameter tuning features.\r\n",
        "\r\n",
        "[Hyperparameter tuning a model (v2)](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters?view=azureml-api-2) documentation was used to make this notebook. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create handle to workspace\r\n",
        "\r\n",
        "Before we dive in the code, you need a way to reference your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\r\n",
        "\r\n",
        "You'll create `ml_client` for a handle to the workspace.  You'll then use `ml_client` to manage resources and jobs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\r\n",
        "from azure.identity import DefaultAzureCredential\r\n",
        "\r\n",
        "# authenticate\r\n",
        "credential = DefaultAzureCredential()\r\n",
        "\r\n",
        "# Get a handle to the workspace\r\n",
        "ml_client = MLClient(\r\n",
        "    credential=credential,\r\n",
        "    subscription_id=\"62718086-f139-4a3d-bf14-d9004762decf\",\r\n",
        "    resource_group_name=\"19121015-rg\",\r\n",
        "    workspace_name=\"vici_2023\",\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688485766638
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training script\r\n",
        "\r\n",
        "Let's start by creating the training script - the *main.py* Python file.\r\n",
        "\r\n",
        "First create a source folder for the script:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "train_src_dir = \"./src\"\r\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688486044374
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script handles the preprocessing of the data, splitting it into test and train data. It then consumes this data to train a tree based model and return the output model. \r\n",
        "\r\n",
        "[Hyperparameter tuning example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/lightgbm/iris/src/main.py) was used as the source of the script. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/main.py\r\n",
        "\r\n",
        "# imports\r\n",
        "import os\r\n",
        "import mlflow\r\n",
        "import argparse\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import lightgbm as lgbm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from sklearn.metrics import log_loss, accuracy_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# define functions\r\n",
        "def main(args):\r\n",
        "    # enable auto logging\r\n",
        "    mlflow.autolog()\r\n",
        "\r\n",
        "    # setup parameters\r\n",
        "    num_boost_round = args.num_boost_round\r\n",
        "    params = {\r\n",
        "        \"objective\": \"multiclass\",\r\n",
        "        \"num_class\": 3,\r\n",
        "        \"boosting\": args.boosting,\r\n",
        "        \"num_iterations\": args.num_iterations,\r\n",
        "        \"num_leaves\": args.num_leaves,\r\n",
        "        \"num_threads\": args.num_threads,\r\n",
        "        \"learning_rate\": args.learning_rate,\r\n",
        "        \"metric\": args.metric,\r\n",
        "        \"seed\": args.seed,\r\n",
        "        \"verbose\": args.verbose,\r\n",
        "    }\r\n",
        "\r\n",
        "    # read in data\r\n",
        "    df = pd.read_csv(args.iris_csv)\r\n",
        "\r\n",
        "    # process data\r\n",
        "    X_train, X_test, y_train, y_test, enc = process_data(df)\r\n",
        "\r\n",
        "    # train model\r\n",
        "    model = train_model(params, num_boost_round, X_train, X_test, y_train, y_test)\r\n",
        "\r\n",
        "\r\n",
        "def process_data(df):\r\n",
        "    # split dataframe into X and y\r\n",
        "    X = df.drop([\"species\"], axis=1)\r\n",
        "    y = df[\"species\"]\r\n",
        "\r\n",
        "    # encode label\r\n",
        "    enc = LabelEncoder()\r\n",
        "    y = enc.fit_transform(y)\r\n",
        "\r\n",
        "    # train/test split\r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "        X, y, test_size=0.2, random_state=42\r\n",
        "    )\r\n",
        "\r\n",
        "    # return splits and encoder\r\n",
        "    return X_train, X_test, y_train, y_test, enc\r\n",
        "\r\n",
        "\r\n",
        "def train_model(params, num_boost_round, X_train, X_test, y_train, y_test):\r\n",
        "    # create lightgbm datasets\r\n",
        "    train_data = lgbm.Dataset(X_train, label=y_train)\r\n",
        "    test_data = lgbm.Dataset(X_test, label=y_test)\r\n",
        "\r\n",
        "    # train model\r\n",
        "    model = lgbm.train(\r\n",
        "        params,\r\n",
        "        train_data,\r\n",
        "        num_boost_round=num_boost_round,\r\n",
        "        valid_sets=[test_data],\r\n",
        "        valid_names=[\"test\"],\r\n",
        "    )\r\n",
        "\r\n",
        "    # return model\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def parse_args():\r\n",
        "    # setup arg parser\r\n",
        "    parser = argparse.ArgumentParser()\r\n",
        "\r\n",
        "    # add arguments\r\n",
        "    parser.add_argument(\"--iris-csv\", type=str)\r\n",
        "    parser.add_argument(\"--num-boost-round\", type=int, default=10)\r\n",
        "    parser.add_argument(\"--boosting\", type=str, default=\"gbdt\")\r\n",
        "    parser.add_argument(\"--num-iterations\", type=int, default=16)\r\n",
        "    parser.add_argument(\"--num-leaves\", type=int, default=31)\r\n",
        "    parser.add_argument(\"--num-threads\", type=int, default=0)\r\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=0.1)\r\n",
        "    parser.add_argument(\"--metric\", type=str, default=\"multi_logloss\")\r\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\r\n",
        "    parser.add_argument(\"--verbose\", type=int, default=0)\r\n",
        "\r\n",
        "    # parse args\r\n",
        "    args = parser.parse_args()\r\n",
        "\r\n",
        "    # return args\r\n",
        "    return args\r\n",
        "\r\n",
        "\r\n",
        "# run script\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    # parse args\r\n",
        "    args = parse_args()\r\n",
        "\r\n",
        "    # run main function\r\n",
        "    main(args)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./src/main.py\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in this script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints.\r\n",
        "\r\n",
        "You might need to select **Refresh** to see the new folder and script in your **Files**.\r\n",
        "\r\n",
        "\r\n",
        "## Create a compute cluster, a scalable way to run a training job\r\n",
        "\r\n",
        "You already have a compute instance, which you're using to run the notebook.  Now you'll add a second type of compute, a **compute cluster** that you'll use to run your training job. While a compute instance is a single node machine, a compute cluster can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\r\n",
        "\r\n",
        "You'll provision a Linux compute cluster. See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\r\n",
        "\r\n",
        "# Name assigned to the compute cluster\r\n",
        "cpu_compute_target = \"cpu-cluster\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # let's see if the compute target already exists\r\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\r\n",
        "    print(\r\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\r\n",
        "    )\r\n",
        "\r\n",
        "except Exception:\r\n",
        "    print(\"Creating a new cpu compute target...\")\r\n",
        "\r\n",
        "    # Let's create the Azure Machine Learning compute object with the intended parameters\r\n",
        "    # if you run into an out of quota error, change the size to a comparable VM that is available.\r\n",
        "    # Learn more on https://azure.microsoft.com/en-us/pricing/details/machine-learning/.\r\n",
        "    cpu_cluster = AmlCompute(\r\n",
        "        name=cpu_compute_target,\r\n",
        "        # Azure Machine Learning Compute is the on-demand VM service\r\n",
        "        type=\"amlcompute\",\r\n",
        "        # VM Family\r\n",
        "        size=\"STANDARD_DS3_V2\",\r\n",
        "        # Minimum running nodes when there is no job running\r\n",
        "        min_instances=0,\r\n",
        "        # Nodes in cluster\r\n",
        "        max_instances=4,\r\n",
        "        # How many seconds will the node running after the job termination\r\n",
        "        idle_time_before_scale_down=180,\r\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\r\n",
        "        tier=\"Dedicated\",\r\n",
        "    )\r\n",
        "    print(\r\n",
        "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\r\n",
        "    )\r\n",
        "    # Now, we pass the object to MLClient's create_or_update method\r\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688486397476
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the command\r\n",
        "\r\n",
        "Now that you have a script that can perform the desired tasks, and a compute cluster to run the script, you'll use a general purpose **command** that can run command line actions. This command line action can directly call system commands or run a script. \r\n",
        "\r\n",
        "## Configure hyperparameter tuning experiment\r\n",
        "\r\n",
        "To configure your hyperparameter tuning experiment, provide the following:\r\n",
        "\r\n",
        "- The defined hyperparameter search space\r\n",
        "- Your sampling algorithm\r\n",
        "- Your early termination policy\r\n",
        "- Your objective\r\n",
        "- Resource limits\r\n",
        "- CommandJob or CommandComponent\r\n",
        "- SweepJob\r\n",
        "- SweepJob can run a hyperparameter sweep on the Command or Command Component."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\r\n",
        "from azure.ai.ml import command, Input\r\n",
        "from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy\r\n",
        "from azure.identity import DefaultAzureCredential\r\n",
        "\r\n",
        "# Create your base command job\r\n",
        "command_job = command(\r\n",
        "    code=\"./src\",\r\n",
        "    command=\"python main.py --iris-csv ${{inputs.iris_csv}} --learning-rate ${{inputs.learning_rate}} --boosting ${{inputs.boosting}}\",\r\n",
        "    environment=\"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu@latest\",\r\n",
        "    inputs={\r\n",
        "        \"iris_csv\": Input(\r\n",
        "            type=\"uri_file\",\r\n",
        "            path=\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\r\n",
        "        ),\r\n",
        "        \"learning_rate\": 0.9,\r\n",
        "        \"boosting\": \"gbdt\",\r\n",
        "    },\r\n",
        "    compute=\"cpu-cluster\",\r\n",
        ")\r\n",
        "\r\n",
        "# Override your inputs with parameter expressions\r\n",
        "command_job_for_sweep = command_job(\r\n",
        "    learning_rate=Uniform(min_value=0.01, max_value=0.9),\r\n",
        "    boosting=Choice(values=[\"gbdt\", \"dart\"]),\r\n",
        ")\r\n",
        "\r\n",
        "# Call sweep() on your command job to sweep over your parameter expressions\r\n",
        "sweep_job = command_job_for_sweep.sweep(\r\n",
        "    compute=\"cpu-cluster\",\r\n",
        "    sampling_algorithm=\"random\",\r\n",
        "    primary_metric=\"test-multi_logloss\",\r\n",
        "    goal=\"Minimize\",\r\n",
        ")\r\n",
        "\r\n",
        "# Specify your experiment details\r\n",
        "sweep_job.display_name = \"lightgbm-iris-sweep-example\"\r\n",
        "sweep_job.experiment_name = \"lightgbm-iris-sweep-example\"\r\n",
        "sweep_job.description = \"Run a hyperparameter sweep job for LightGBM on Iris dataset.\"\r\n",
        "\r\n",
        "# Define the limits for this sweep\r\n",
        "sweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\r\n",
        "\r\n",
        "# Set early stopping on this one\r\n",
        "sweep_job.early_termination = MedianStoppingPolicy(\r\n",
        "    delay_evaluation=5, evaluation_interval=2\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688486855307
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit hyperparameter tuning experiment\r\n",
        "\r\n",
        "It's now time to submit the job to run in Azure Machine Learning. This time you'll use `create_or_update`  on `ml_client`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the sweep\r\n",
        "returned_sweep_job = ml_client.create_or_update(sweep_job)\r\n",
        "# get a URL for the status of the job\r\n",
        "returned_sweep_job.services[\"Studio\"].endpoint"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading src (0.0 MBs):   0%|          | 0/2826 [00:00<?, ?it/s]\r\u001b[32mUploading src (0.0 MBs): 100%|██████████| 2826/2826 [00:00<00:00, 1227029.31it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'https://ml.azure.com/runs/serene_farm_50j428mn5h?wsid=/subscriptions/62718086-f139-4a3d-bf14-d9004762decf/resourcegroups/19121015-rg/workspaces/vici_2023&tid=3db1f987-edf3-43db-88eb-43a0395f7c84'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688486950358
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}